{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN1yUXlo8GGewlTUbY3wR0Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EmmaTopolovec/AlexaControlledLEDs/blob/main/assign1_topolovec_B00879163.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CS 580E Assignment 1\n",
        "**Emma Topolovec | B00879163**\n"
      ],
      "metadata": {
        "id": "adZMhOCon00h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "##Part 1 [16 pts]: Numerical Inversion by Binary Search\n",
        "\n",
        "For this part, you are to write a function named invert that has two required parameters and one optional parameter d. The required parameters are another function f(x) and a y value. Your function should return the x value such that f(x) = y. The function is guaranteed to be non-decreasing, i.e., the derivative is always 0 or positive. If a 3rd argument is given, it will be a tuple (if using Python) representing an interval where the function is guaranteed to be non-decreasing. Otherwise, you may assume that the function is everywhere non-decreasing.\n",
        "\n",
        "Make sure your function always terminates.\n",
        "\n",
        "Place your function in a separate cell.\n",
        "\n",
        "You will then apply your invert function to the two functions below."
      ],
      "metadata": {
        "id": "P4bJRMC-6dJX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "G2hZk8rb6HWF"
      },
      "outputs": [],
      "source": [
        "# @title Part 1 Function\n",
        "import math\n",
        "\n",
        "def invert(f, y, d=None):\n",
        "  # Initialize low and high\n",
        "  low = 0\n",
        "  high = 0\n",
        "  mid = -1\n",
        "  prev_mid = -1\n",
        "\n",
        "  # If d is not None, then d has the low and high\n",
        "  if d != None:\n",
        "    low = d[0]\n",
        "    high = d[1]\n",
        "\n",
        "    # Error check: if y is not within f(low) and f(high), then invalid input\n",
        "    if y > f(high) or y < f(low):\n",
        "      raise Exception('Invalid Input: given y value is not within the given range')\n",
        "\n",
        "  # Else, loop exponentially until f(low) < y and f(high) > y\n",
        "  else:\n",
        "    low = -1\n",
        "    high = 1\n",
        "    while f(low) > y or f(high) < y:\n",
        "      low *= 10\n",
        "      high *= 10\n",
        "\n",
        "  # Now, perform binary search\n",
        "  while low <= high:\n",
        "\n",
        "    # Set mid\n",
        "    mid = (high + low) / 2.0\n",
        "\n",
        "    v = f(mid)\n",
        "\n",
        "    \"\"\"\n",
        "    To get the most precision, we first try to check if the predicted\n",
        "    inverse yields our target y exactly. If not, we keep looping. If at any\n",
        "    point, our mid value stays the same, then we can switch over to\n",
        "    checking if the values are within 1e-15 apart.\n",
        "    \"\"\"\n",
        "\n",
        "    if mid == prev_mid:\n",
        "      # If f(mid) is equal to y --> mid is x\n",
        "      if math.isclose(v, y, rel_tol=1e-15):\n",
        "        return(mid)\n",
        "    elif v == y:\n",
        "      return(mid)\n",
        "\n",
        "    # Update prev_mid\n",
        "    prev_mid = mid\n",
        "\n",
        "    # If f(mid) is less than y, ignore lower half\n",
        "    if v < y:\n",
        "      low = mid\n",
        "    # If f(mid) is greater than y, ignore upper half\n",
        "    elif v > y:\n",
        "      high = mid\n",
        "\n",
        "  # Element not found? should never trigger\n",
        "  return(mid)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 1.1 [8 pts]: Transcendental Functions**\n",
        "\n",
        "Apply your invert function to this function:\n",
        "\n",
        "f(x) = x<sup>e</sup> + x\n",
        "\n",
        "In a separate cell, write a test to make sure that your invert routine is able to invert the function everywhere. We leave it up to you to think about how to design and implement such a test."
      ],
      "metadata": {
        "id": "43k87bnGoRkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Part 1.1 Tests\n",
        "\n",
        "test_values = [-19902.3231, -100, -1, -0.000001, 0, .000001, 1, 100, 10993.234, 992910492.123424]\n",
        "\n",
        "for y in test_values:\n",
        "  try:\n",
        "    inverse = invert(lambda x : x**math.e + x, y, (0, 1e9)) # Find inverse - We MUST give the function an interval as it does not exist when x < 0\n",
        "    predicted_y = inverse**math.e + inverse\n",
        "\n",
        "    print(f\"Inverse of {y}: {inverse} - Loss of precision: {abs(y - predicted_y)}\")\n",
        "  except Exception as e: # No inverse can be found\n",
        "    print(f\"No inverse of {y} - {e}\")"
      ],
      "metadata": {
        "id": "SC-tDE8pdJ45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 1.2 [8 pts]: Gamma Function**\n",
        "\n",
        "The gamma function is component of many probability distribution functions and has a wide range of applications in probability, statistics, and physics. The gamma function is defined as an improper integral:\n",
        "\n",
        "Γ(x) = ∫<sup>∞</sup><sub>0</sub> t<sup>x-1</sup>e<sup>-t</sup>dt\n",
        "\n",
        "In a new cell, write a test to make that your invert routine is able to invert the Gamma function in the domain (1.5, 16). You may use the gamma function defined in SciPy."
      ],
      "metadata": {
        "id": "y1gSlB0So0By"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Part 1.2 Tests\n",
        "\n",
        "import scipy\n",
        "\n",
        "test_values = [-19902.3231, -2, 0, 1, 1.5, 2, 2.4532, 12.4532, 15.9999, 16, 100, 110.32]\n",
        "\n",
        "for y in test_values:\n",
        "  try:\n",
        "    inverse = invert(scipy.special.gamma, y, (1.5, 16)) # Find inverse\n",
        "    predicted_y = scipy.special.gamma(inverse)\n",
        "\n",
        "    print(f\"Inverse of {y}: {inverse} - Loss of precision: {abs(y - predicted_y)}\")\n",
        "  except Exception as e: # No inverse can be found\n",
        "    print(f\"No inverse of {y} - {e}\")"
      ],
      "metadata": {
        "id": "9EE-Opp2oxKU",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "##Part 2 [7 pts]: Numerical Integration\n",
        "\n",
        "A large class of continuous functions do not have closed-form solutions for their antiderivatives. In this case, the definite integral over a range can be computed numerically. Write a function integrate that has two parameters: another function f(x) and a tuple representing the range of integration. Your function should return the definite integral of the function f(x), computed over the given range.\n",
        "\n",
        "Place your function in a separate cell.\n",
        "\n",
        "In the following cell define the function below, and compute the definite integral numerically using your integrate function.\n",
        "\n",
        "∫<sub>-1</sub><sup>1</sup>(sin(x)/x)dx\n",
        "\n",
        "You may use Monte Carlo or other sampling-based methods, as long as the program terminates in a reasonable amount of time. Your function should return the integral value with as high a precision as possible."
      ],
      "metadata": {
        "id": "dmwtMN5Gxb5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Part 2 Function\n",
        "\n",
        "def integrate(f, d):\n",
        "  # Trapezoidal Riemann Sums\n",
        "  integral = 0\n",
        "  samples = 100000\n",
        "  x_range = d[1]-d[0]\n",
        "  trap_width = x_range / float(samples)\n",
        "  for i in range(samples+1):\n",
        "    if i == 0 or i == samples: # first or last trapezoid\n",
        "      integral += f(d[0] + trap_width * i)\n",
        "    else:\n",
        "      integral += 2 * f(d[0] + trap_width * i)\n",
        "  return integral / float(samples)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "G29oTVXkx7Oc"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Part 2 Test\n",
        "\n",
        "import math\n",
        "\n",
        "def f_part2(x):\n",
        "  if x == 0: # Can't divide by 0\n",
        "    return(1)\n",
        "  return(math.sin(x)/x)\n",
        "\n",
        "print(f\"The integral of sin(x)/x from -1 to 1 is {integrate(f_part2, (-1, 1))}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "KoK1UWCP0LJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "##Part 3 [39 pts]: Gradient Descent\n",
        "\n",
        "For this part, you will write a gradient descent optimizer and use it to minimize some functions. In the below, we’ll call the function to be optimized f. Write a function opt with three parameters: a function grad(p), a tuple start giving the starting point of the optimization, and the step size step. The function grad(p) should accept a point p as a tuple and returns a tuple giving the gradient of f at p.\n",
        "\n",
        "Your function opt should return an array of tuples consisting of the sequence of points where the gradient is evaluated, starting from the start point. The last element of the returned array should be the point that minimizes f. The point may be a local mininum.\n",
        "\n",
        "Note: Make sure your function always terminates.\n",
        "\n",
        "Place your function in a separate cell. You will then apply your opt function to two functions below."
      ],
      "metadata": {
        "id": "PXXcUSCR2qLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Part 3 Function\n",
        "\n",
        "import math\n",
        "\n",
        "def opt(grad, start, step_size):\n",
        "  # List to store the sequence of points\n",
        "  points = [start]\n",
        "\n",
        "  # Current point\n",
        "  curr_p = start\n",
        "\n",
        "  i = 0\n",
        "  while True:\n",
        "    i += 1\n",
        "\n",
        "    # Compute the gradient at the current point\n",
        "    gradient = grad(curr_p)\n",
        "\n",
        "    # Update the current point using gradient descent\n",
        "    curr_p = tuple(p - step_size * g for p, g in zip(curr_p, gradient))\n",
        "\n",
        "    # If the point is already in the sequence, then we will be stuck in an infinite loop\n",
        "    # Only do this sometimes to be efficient\n",
        "    if i % 10000 == 0 and curr_p in points:\n",
        "      points.append(curr_p)\n",
        "      return points\n",
        "\n",
        "    # Add the new point to the sequence\n",
        "    points.append(curr_p)\n",
        "\n",
        "    # Check for convergence (if the gradient is close to zero)\n",
        "    if math.isclose(math.sqrt(gradient[0]**2 + gradient[1]**2), 0, rel_tol=1e-6):\n",
        "        break\n",
        "\n",
        "  return points"
      ],
      "metadata": {
        "id": "Gzn8aiVV2w7L"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 3.1 [15 pts]: Three-Hump Camel Function**\n",
        "\n",
        "The three-hump camel function is a two-dimensional function often used to benchmark optimization methods. It has multiple extrema. The three-hump camel function is defined as:\n",
        "\n",
        "f(x<sub>1</sub>, x<sub>2</sub>) = 2x<sub>1</sub><sup>2</sup> - 1.05x<sub>1</sub><sup>4</sup> + x<sub>1</sub><sup>6</sup>/6 + x<sub>1</sub>x<sub>2</sub> + x<sub>2</sub><sup>2</sup>\n",
        "\n",
        "where, -5 <= x<sub>i</sub> <= 5, i = 1,2\n",
        "\n",
        "\n",
        "1.   Implement the function thcf with a single tuple parameter p, giving the coordinates x1 and x2 to evaluate the function.\n",
        "2.   Implement the gradient function, thcf_grad that accepts a point and returns the gradient of that point, both as tuples.\n",
        "3.    In this cell, put in code that checks using a finite differnce that your thcf_grad function is implemented correctly. It should use randomization of the point at which to check the gradient, and allow running for any number of checks. It should be as convincing as possible.\n",
        "4.    Find the global minimum, possibly using multiple runs of your opt function. Your code should not have a hard-coded starting point.\n",
        "5.    Generate a surface plot or contour plot of the function, showing the path of the gradient descent that found the global minimum. You can use the matplotlib library."
      ],
      "metadata": {
        "id": "uxGlsrlrMNck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Part 3.1 Code\n",
        "\n",
        "import random\n",
        "\n",
        "# 1 - Three-Hump Camel Function\n",
        "def thcf(p):\n",
        "  x1 = p[0]\n",
        "  x2 = p[1]\n",
        "\n",
        "  # If x1 or x2 are out of range, put them in the range\n",
        "  if x1 > 5:\n",
        "    x1 = 5\n",
        "  elif x1 < -5:\n",
        "    x1 = -5\n",
        "  if x2 > 5:\n",
        "    x2 = 5\n",
        "  elif x2 < -5:\n",
        "    x2 = -5\n",
        "\n",
        "  return (2 * x1**2 - 1.05 * x1**4 + x1**6 / 6 + x1 * x2 + x2**2)\n",
        "\n",
        "# 2 - Three-Hump Camel Gradient Function\n",
        "def thcf_grad(p):\n",
        "  x1 = p[0]\n",
        "  x2 = p[1]\n",
        "\n",
        "  # If x1 or x2 are out of range, put them in the range\n",
        "  if x1 > 5:\n",
        "    x1 = 5\n",
        "  elif x1 < -5:\n",
        "    x1 = -5\n",
        "  if x2 > 5:\n",
        "    x2 = 5\n",
        "  elif x2 < -5:\n",
        "    x2 = -5\n",
        "\n",
        "  dx1 = 4*x1 - 4*1.05*x1**3 + x1**5 + x2\n",
        "  dx2 = x1 + 2*x2\n",
        "  return (dx1, dx2)\n",
        "\n",
        "# 3 - Finite difference to test thcf_grad()\n",
        "def finite_difference(n=1000, epsilon=1e-9):\n",
        "  print(\"Running finite_difference() function to test thcf_grad()\")\n",
        "  # random.seed(0)\n",
        "  passes = 0\n",
        "  for test_num in range(n):\n",
        "    # Generate random point within range\n",
        "    p = (random.uniform(-5, 5), random.uniform(-5, 5))\n",
        "\n",
        "    # Find exact gradient\n",
        "    gradient = thcf_grad(p)\n",
        "\n",
        "    # Compute finite difference for both dimension\n",
        "    a1 = (p[0] + epsilon, p[1])\n",
        "    b1 = (p[0] - epsilon, p[1])\n",
        "    a2 = (p[0], p[1] + epsilon)\n",
        "    b2 = (p[0], p[1] - epsilon)\n",
        "\n",
        "    # Find approximate gradient for both dimensions\n",
        "    approx_gradient = ( (thcf(a1) - thcf(b1)) / (2 * epsilon),\n",
        "                        (thcf(a2) - thcf(b2)) / (2 * epsilon) )\n",
        "\n",
        "    # Check if the actual gradient is close to the approximate\n",
        "    x1_error = abs((gradient[0] - approx_gradient[0]) / gradient[0])\n",
        "    x2_error = abs((gradient[1] - approx_gradient[1]) / gradient[1])\n",
        "    error_allowed = .01 # 1 percent error allowed\n",
        "    if x1_error < error_allowed and x2_error < error_allowed:\n",
        "      passes += 1\n",
        "    else:\n",
        "      print(f\"\\n! Gradient check failed for {p}\")\n",
        "      print(f\"! Percent Error:({x1_error*100}, {x2_error*100})\")\n",
        "      print(f\"! Actual gradient - {gradient}\")\n",
        "      print(f\"! Approx gradient - {approx_gradient}\")\n",
        "\n",
        "  print(f\"{passes}/{n} tests passed.\\n\")\n",
        "\n",
        "finite_difference()\n",
        "\n",
        "# 4 - Global Minimum of Three-Hump Camel Function\n",
        "def find_thcf_min():\n",
        "  print(\"Running find_thcf_min() function to find the minimum\")\n",
        "  # random.seed(0)\n",
        "  min_point = None\n",
        "  min_val = None\n",
        "  samples = 50\n",
        "  for i in range(samples):\n",
        "    # Generate random point to start\n",
        "    p = (random.uniform(-5, 5), random.uniform(-5, 5))\n",
        "\n",
        "    # Get min - use random step size\n",
        "    step_size = random.uniform(0.0001, .05)\n",
        "    curr_min_point = opt(thcf_grad, p, step_size)[-1]\n",
        "    curr_min_val = thcf(curr_min_point)\n",
        "\n",
        "    # Update global minimum if the current minimum is smaller\n",
        "    if min_val == None or curr_min_val < min_val:\n",
        "      min_val = curr_min_val\n",
        "      min_point = curr_min_point\n",
        "\n",
        "  # Return the minimum minimum\n",
        "  return(min_point, min_val)\n",
        "\n",
        "thcf_min = find_thcf_min()\n",
        "print(f\"Global minimum: f{thcf_min[0]} = {thcf_min[1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwH6Jvf4MLCP",
        "outputId": "46a64c23-0daf-4027-baa1-e65a6926f95f"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running finite_difference() function to test thcf_grad()\n",
            "1000/1000 tests passed.\n",
            "\n",
            "Running find_thcf_min() function to find the minimum\n",
            "Global minimum: f(4.0247393355423205e-163, -9.716580288882752e-163) = 0.0\n"
          ]
        }
      ]
    }
  ]
}